{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21437db7",
   "metadata": {},
   "source": [
    "\n",
    "### **UNIT 4:**\n",
    "\n",
    "* Stereo and Multi-view Reconstruction\n",
    "* Structure-from-Motion\n",
    "* Projection Matrices\n",
    "* Camera Calibration\n",
    "* Epipolar Geometry\n",
    "* Fundamental and Essential Matrices\n",
    "* Disparity Maps\n",
    "* Optical Flows\n",
    "* Volumetric Shape Reconstruction\n",
    "\n",
    "  * From Window-based to Regularization-based Stereo\n",
    "* Loss Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d6c88",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **UNIT 4: Stereo and Multi-view Reconstruction**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Stereo and Multi-view Reconstruction**\n",
    "\n",
    "**Concept:**\n",
    "Stereo vision and multi-view reconstruction are techniques that recover **3D information** from **2D images** captured from different viewpoints.\n",
    "\n",
    "* **Stereo Vision:** Uses **two cameras** (like human eyes) to estimate depth.\n",
    "* **Multi-view Reconstruction:** Extends stereo vision to **many cameras or frames** (like video frames) to reconstruct 3D structure of a scene.\n",
    "\n",
    "**Goal:**\n",
    "To compute **depth (Z-coordinate)** of points and build a **3D model** of the scene.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "1. **Image capture** – Take images from different viewpoints.\n",
    "2. **Feature matching** – Find corresponding points in different images.\n",
    "3. **Epipolar geometry** – Use geometry of two cameras to constrain matches.\n",
    "4. **Triangulation** – Compute 3D coordinates using matched points.\n",
    "5. **Reconstruction** – Build full 3D scene or object.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* 3D mapping\n",
    "* Robotics and autonomous driving\n",
    "* Virtual/augmented reality\n",
    "* Medical imaging and 3D scanning\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Structure-from-Motion (SfM)**\n",
    "\n",
    "**Definition:**\n",
    "Structure-from-Motion (SfM) is the process of **estimating 3D structure** and **camera motion** simultaneously from a sequence of 2D images.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Feature Detection:** Detect keypoints (SIFT, SURF, ORB, etc.)\n",
    "2. **Feature Matching:** Match keypoints between multiple images.\n",
    "3. **Camera Pose Estimation:** Compute relative position and orientation of cameras.\n",
    "4. **Triangulation:** Compute 3D coordinates of matched points.\n",
    "5. **Bundle Adjustment:** Optimize all camera parameters and 3D points to minimize re-projection error.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "* Sparse 3D point cloud\n",
    "* Camera positions and orientations\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* 3D scene reconstruction\n",
    "* Drone mapping\n",
    "* AR/VR environment creation\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Projection Matrices**\n",
    "\n",
    "**Definition:**\n",
    "A **projection matrix** defines how a **3D world point** is projected onto a **2D image plane**.\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\n",
    "s\n",
    "\\begin{bmatrix}\n",
    "u \\ v \\ 1\n",
    "\\end{bmatrix}\n",
    "=============\n",
    "\n",
    "P\n",
    "\\begin{bmatrix}\n",
    "X \\ Y \\ Z \\ 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $(X, Y, Z)$ = 3D world coordinates\n",
    "* $(u, v)$ = 2D image coordinates\n",
    "* $s$ = scale factor\n",
    "* $P$ = 3×4 projection matrix\n",
    "\n",
    "**Camera Projection Matrix:**\n",
    "$$\n",
    "P = K [R | t]\n",
    "$$\n",
    "\n",
    "* **K** = intrinsic parameters (focal length, optical center)\n",
    "* **R** = rotation matrix\n",
    "* **t** = translation vector\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Camera Calibration**\n",
    "\n",
    "**Purpose:**\n",
    "Camera calibration finds the **intrinsic** and **extrinsic** parameters of the camera.\n",
    "\n",
    "**A. Intrinsic Parameters (inside camera):**\n",
    "\n",
    "* Focal length ($f_x, f_y$)\n",
    "* Optical center ($c_x, c_y$)\n",
    "* Skew (angle between x and y axes)\n",
    "* Distortion coefficients (lens distortion)\n",
    "\n",
    "**Intrinsic matrix (K):**\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "f_x & \\gamma & c_x\\\n",
    "0 & f_y & c_y\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**B. Extrinsic Parameters (camera pose):**\n",
    "\n",
    "* Rotation (R)\n",
    "* Translation (t)\n",
    "\n",
    "They define how the camera is placed in the world.\n",
    "\n",
    "**Calibration Process:**\n",
    "\n",
    "1. Capture images of a known pattern (like chessboard).\n",
    "2. Detect corners in all images.\n",
    "3. Solve for K, R, and t using optimization (e.g., Zhang’s method).\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* 3D reconstruction\n",
    "* Robotics\n",
    "* AR/VR alignment\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Epipolar Geometry**\n",
    "\n",
    "**Definition:**\n",
    "Epipolar geometry is the **geometric relationship** between **two cameras** that view the same 3D scene.\n",
    "\n",
    "**Key Terms:**\n",
    "\n",
    "* **Epipole (e, e’):** Projection of one camera center onto the other’s image plane.\n",
    "* **Epipolar Line:** Line along which the matching point in the other image must lie.\n",
    "* **Epipolar Plane:** Plane passing through both camera centers and a 3D point.\n",
    "\n",
    "**Property:**\n",
    "A point in one image corresponds to an **epipolar line** in the other image.\n",
    "This reduces matching search from 2D → 1D.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Fundamental and Essential Matrices**\n",
    "\n",
    "### **A. Fundamental Matrix (F):**\n",
    "\n",
    "Defines the epipolar constraint between two images.\n",
    "\n",
    "Equation:\n",
    "$$\n",
    "x'^T F x = 0\n",
    "$$\n",
    "where\n",
    "\n",
    "* $x$ = point in first image (in homogeneous form)\n",
    "* $x'$ = corresponding point in second image\n",
    "\n",
    "**F** is a 3×3 matrix of rank 2 and is valid for **uncalibrated cameras**.\n",
    "\n",
    "**Computation:**\n",
    "Estimated using **8-point algorithm** or **normalized 8-point algorithm**.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Essential Matrix (E):**\n",
    "\n",
    "Used for **calibrated cameras** (known intrinsic parameters).\n",
    "\n",
    "Relation between Fundamental and Essential matrices:\n",
    "$$\n",
    "E = K'^T F K\n",
    "$$\n",
    "**E** also satisfies:\n",
    "$$\n",
    "x'^T E x = 0\n",
    "$$\n",
    "\n",
    "**Decomposition of E:**\n",
    "\n",
    "* Extracts rotation (R) and translation (t) between the two cameras.\n",
    "* Used for stereo calibration and pose estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Disparity Maps**\n",
    "\n",
    "**Concept:**\n",
    "Disparity represents the difference in the horizontal position of corresponding points in the left and right images.\n",
    "\n",
    "$$\n",
    "\\text{Disparity} = x_L - x_R\n",
    "$$\n",
    "\n",
    "**Depth Estimation:**\n",
    "Depth is inversely proportional to disparity:\n",
    "$$\n",
    "Z = \\frac{fB}{d}\n",
    "$$\n",
    "where\n",
    "\n",
    "* $f$ = focal length\n",
    "* $B$ = baseline distance between cameras\n",
    "* $d$ = disparity\n",
    "\n",
    "**Disparity Map:**\n",
    "A grayscale image showing disparity values for every pixel.\n",
    "Brighter areas = closer objects, darker = farther away.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* Depth estimation\n",
    "* 3D modeling\n",
    "* Self-driving vehicles\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Optical Flow**\n",
    "\n",
    "**Definition:**\n",
    "Optical flow measures **motion of pixels** between two consecutive frames in a video.\n",
    "\n",
    "**Assumption:**\n",
    "\n",
    "* Brightness of a pixel remains constant during motion.\n",
    "\n",
    "**Optical Flow Equation:**\n",
    "$$\n",
    "I_x u + I_y v + I_t = 0\n",
    "$$\n",
    "where\n",
    "\n",
    "* $I_x, I_y$ = spatial derivatives\n",
    "* $I_t$ = temporal derivative\n",
    "* $(u, v)$ = pixel velocity components (flow vector)\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "1. **Lucas–Kanade Method:** Local (block-based) approach using least squares.\n",
    "2. **Horn–Schunck Method:** Global approach using smoothness constraints.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* Motion detection\n",
    "* Object tracking\n",
    "* Video stabilization\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Volumetric Shape Reconstruction**\n",
    "\n",
    "**Concept:**\n",
    "Builds a **3D volumetric model** from multiple 2D images or depth maps.\n",
    "\n",
    "**Approaches:**\n",
    "\n",
    "1. **Voxel-based (volume element) Reconstruction:**\n",
    "\n",
    "   * Divide space into small cubes (voxels).\n",
    "   * Label each voxel as inside or outside the object using silhouettes or depth data.\n",
    "2. **Surface-based Reconstruction:**\n",
    "\n",
    "   * Uses point clouds and fits surfaces (like meshes) through them.\n",
    "   * Example: Poisson surface reconstruction.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* 3D scanning\n",
    "* CAD modeling\n",
    "* Medical CT/MRI reconstruction\n",
    "\n",
    "---\n",
    "\n",
    "### **From Window-based to Regularization-based Stereo**\n",
    "\n",
    "**1. Window-based Stereo:**\n",
    "\n",
    "* Matches image patches (windows) between left and right images.\n",
    "* Uses metrics like SAD (Sum of Absolute Differences) or SSD (Sum of Squared Differences).\n",
    "* Simple but may fail near edges or texture-less areas.\n",
    "\n",
    "**2. Regularization-based Stereo:**\n",
    "\n",
    "* Adds smoothness or prior constraints to improve accuracy.\n",
    "* Uses energy minimization:\n",
    "  $$\n",
    "  E(D) = E_{data}(D) + \\lambda E_{smooth}(D)\n",
    "  $$\n",
    "\n",
    "  * **E_data:** matching cost\n",
    "  * **E_smooth:** penalizes large disparity jumps\n",
    "  * **λ:** controls smoothness strength\n",
    "* Solved using graph cuts, belief propagation, or dynamic programming.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Loss Functions in Reconstruction**\n",
    "\n",
    "Loss functions evaluate how well the estimated image or depth matches the ground truth.\n",
    "\n",
    "**Common types:**\n",
    "\n",
    "1. **Photometric Loss:**\n",
    "   $$\n",
    "   L_{photo} = |I_1(x) - I_2(x + d(x))|\n",
    "   $$\n",
    "   Measures brightness difference between matched pixels.\n",
    "\n",
    "2. **Smoothness Loss:**\n",
    "   Encourages smooth disparity/depth maps:\n",
    "   $$\n",
    "   L_{smooth} = |\\nabla D(x)|\n",
    "   $$\n",
    "\n",
    "3. **Structural Similarity (SSIM) Loss:**\n",
    "   Measures perceptual similarity between images.\n",
    "\n",
    "4. **Depth Consistency Loss:**\n",
    "   Ensures reconstructed depth aligns across views.\n",
    "\n",
    "**Total Loss:**\n",
    "$$\n",
    "L = \\alpha L_{photo} + \\beta L_{smooth} + \\gamma L_{SSIM}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| **Topic**                 | **Purpose / Use**                      |\n",
    "| ------------------------- | -------------------------------------- |\n",
    "| Stereo Vision             | Depth from two cameras                 |\n",
    "| Structure-from-Motion     | 3D + camera motion from multiple views |\n",
    "| Projection Matrix         | Maps 3D world to 2D image              |\n",
    "| Camera Calibration        | Find intrinsic & extrinsic parameters  |\n",
    "| Epipolar Geometry         | Geometric link between two cameras     |\n",
    "| Fundamental Matrix        | For uncalibrated stereo pairs          |\n",
    "| Essential Matrix          | For calibrated stereo pairs            |\n",
    "| Disparity Map             | Depth representation                   |\n",
    "| Optical Flow              | Pixel motion tracking                  |\n",
    "| Volumetric Reconstruction | Build 3D models                        |\n",
    "| Regularization Stereo     | Smooth, accurate depth                 |\n",
    "| Loss Functions            | Optimization in 3D reconstruction      |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
