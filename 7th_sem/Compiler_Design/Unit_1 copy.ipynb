{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec0d9b2",
   "metadata": {},
   "source": [
    "## **UNIT I: Introduction to Compilers**\n",
    "\n",
    "* Structure of a Compiler\n",
    "* **Lexical Analysis**\n",
    "\n",
    "  * Role of Lexical Analyzer\n",
    "  * Input Buffering\n",
    "  * Specification of Tokens\n",
    "  * Recognition of Tokens\n",
    "  * **Lex Tool**\n",
    "* **Finite Automata**\n",
    "\n",
    "  * Regular Expressions to Automata\n",
    "  * Minimizing DFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af436c8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# âœ… **UNIT I: Introduction to Compilers**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 1. What is a Compiler?\n",
    "\n",
    "A **compiler** is a special program that translates a source program (written in a high-level language like C, C++, Java) into a target program (machine code or intermediate code).\n",
    "\n",
    "ğŸ‘‰ Example:\n",
    "Source code:\n",
    "\n",
    "```c\n",
    "int sum = a + b;\n",
    "```\n",
    "\n",
    "Compiler translates â†’\n",
    "\n",
    "```\n",
    "MOV R1, a\n",
    "MOV R2, b\n",
    "ADD R1, R2\n",
    "MOV sum, R1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2. Structure of a Compiler\n",
    "\n",
    "The compilation process is divided into **phases**, each handling one specific task.\n",
    "\n",
    "### ğŸ“Œ Phases of Compilation\n",
    "\n",
    "**Front End (Analysis)** â†’ Breaks down source code\n",
    "\n",
    "1. **Lexical Analysis** â€“ Converts characters â†’ tokens  \n",
    "2. **Syntax Analysis** â€“ Checks grammar rules  \n",
    "3. **Semantic Analysis** â€“ Checks meaning (type checking)  \n",
    "\n",
    "**Middle**\n",
    "\n",
    "4. **Intermediate Code Generation** â€“ Produces intermediate representation (IR)  \n",
    "\n",
    "**Back End (Synthesis)** â†’ Produces target code\n",
    "\n",
    "5. **Code Optimization** â€“ Improves efficiency  \n",
    "6. **Code Generation** â€“ Produces machine code  \n",
    "\n",
    "**Extra:**\n",
    "\n",
    "* **Symbol Table Management** â†’ Stores identifiers, functions, variables.\n",
    "* **Error Handling** â†’ Detects & reports errors at all phases.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Diagram (Textual)**\n",
    "\n",
    "```\n",
    "Source Program \n",
    "     â†“\n",
    "Lexical Analysis â†’ Syntax Analysis â†’ Semantic Analysis \n",
    "     â†“\n",
    "Intermediate Code Generation â†’ Optimization â†’ Code Generation \n",
    "     â†“\n",
    "Target Program\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 3. Lexical Analysis\n",
    "\n",
    "### ğŸ“Œ Role of Lexical Analyzer\n",
    "\n",
    "* Reads **characters** of source code â†’ Groups them into **tokens**.\n",
    "* Removes **whitespace, comments**.\n",
    "* Passes tokens to **parser (syntax analyzer)**.\n",
    "* Manages **symbol table** entries.\n",
    "\n",
    "ğŸ‘‰ Example:\n",
    "Code: `int x = 10;`  \n",
    "Tokens:\n",
    "\n",
    "* `int` â†’ keyword\n",
    "* `x` â†’ identifier\n",
    "* `=` â†’ operator\n",
    "* `10` â†’ constant\n",
    "* `;` â†’ delimiter\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Input Buffering\n",
    "\n",
    "Problem: Compiler reads source one character at a time â†’ slow.  \n",
    "Solution: Use **double buffering**:  \n",
    "\n",
    "* Two buffers of equal size.\n",
    "* **Forward pointer** scans input.\n",
    "* **Lexeme begin pointer** marks start of token.\n",
    "* **Sentinels** mark buffer end (avoid checking EOF each time).\n",
    "\n",
    "ğŸ‘‰ Helps speed up reading tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Specification of Tokens\n",
    "\n",
    "A **token** = pair:\n",
    "\n",
    "1. **Token name** â†’ Category (identifier, keyword, operator, constant).\n",
    "2. **Attribute value** â†’ Points to symbol table entry.\n",
    "\n",
    "ğŸ‘‰ Example:\n",
    "Code: `float price = 99;`\n",
    "\n",
    "* `float` â†’ KEYWORD\n",
    "* `price` â†’ IDENTIFIER (points to symbol table entry: type = float)\n",
    "* `99` â†’ CONSTANT\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Recognition of Tokens\n",
    "\n",
    "Lexical analyzer uses **regular expressions (RE)** and **finite automata (FA)**.\n",
    "\n",
    "ğŸ‘‰ Examples of regex:\n",
    "\n",
    "* Identifier: `[a-zA-Z_][a-zA-Z0-9_]*`\n",
    "* Number: `[0-9]+`\n",
    "* Whitespace: `\\t | \\n | \" \"`\n",
    "\n",
    "The FA recognizes input patterns â†’ outputs tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Lex Tool (Lex/Flex)\n",
    "\n",
    "A **tool** that automatically generates lexical analyzers.\n",
    "\n",
    "ğŸ‘‰ Example Lex Program:\n",
    "\n",
    "```lex\n",
    "%{\n",
    "#include <stdio.h>\n",
    "%}\n",
    "%%\n",
    "[0-9]+    { printf(\"NUMBER \"); }\n",
    "[a-zA-Z]+ { printf(\"WORD \"); }\n",
    ".         { printf(\"SYMBOL \"); }\n",
    "%%\n",
    "int main() { yylex(); return 0; }\n",
    "```\n",
    "\n",
    "Input: `sum = 100`\n",
    "Output: `WORD SYMBOL NUMBER`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 4. Finite Automata (FA) in Compiler\n",
    "\n",
    "Lexical analyzer relies on **FA** to recognize tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Regular Expressions â†’ Automata\n",
    "\n",
    "* Regular Expressions describe token patterns.\n",
    "* Automata (FA) recognize them.\n",
    "\n",
    "ğŸ‘‰ Example:\n",
    "RE: `(a|b)*abb` â†’ Strings over `{a,b}` ending with `abb`.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Types of Automata\n",
    "\n",
    "1. **NFA (Non-deterministic FA)**\n",
    "\n",
    "   * Multiple paths possible.\n",
    "   * Easier to build from RE.\n",
    "2. **DFA (Deterministic FA)**\n",
    "\n",
    "   * One unique path for each input symbol.\n",
    "   * Faster execution.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ NFA â†’ DFA Conversion (Subset Construction)\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Start from NFA start state.\n",
    "2. Find epsilon-closures.\n",
    "3. Make DFA states as sets of NFA states.\n",
    "4. Define transitions for each input.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Minimization of DFA\n",
    "\n",
    "* Removes redundant states.\n",
    "* Ensures efficiency.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Separate states into **final** and **non-final** groups.\n",
    "2. Split groups based on transitions.\n",
    "3. Merge equivalent states.\n",
    "\n",
    "ğŸ‘‰ Example: DFA with 6 states may reduce to 3 after minimization.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **UNIT I Summary (Key Points)**\n",
    "\n",
    "* Compiler = Translator (HLL â†’ Machine Code).\n",
    "* Phases: **Lexical â†’ Syntax â†’ Semantic â†’ IR â†’ Optimization â†’ Code Generation**.\n",
    "* **Lexical Analysis**: Converts characters â†’ tokens using **regex + FA**.\n",
    "* **Input Buffering**: Uses double buffer with sentinels.\n",
    "* **Tokens** = name + attribute value (symbol table).\n",
    "* **Lex Tool** automates token recognition.\n",
    "* **Finite Automata**: NFA â†’ DFA â†’ Minimized DFA used in token recognition.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18d77f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ **Lexical Analysis (Compiler Design â€“ Unit I)**\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ **Role of Lexical Analyzer**\n",
    "\n",
    "The **Lexical Analyzer (scanner)** is the **first phase of a compiler**.\n",
    "It reads the **source code character by character** and groups characters into meaningful **lexemes** (words), which it then classifies as **tokens**.\n",
    "\n",
    "ğŸ‘‰ **Example:**\n",
    "Input (source code):\n",
    "\n",
    "```c\n",
    "int sum = a + b;\n",
    "```\n",
    "\n",
    "Output (tokens):\n",
    "\n",
    "* `int` â†’ keyword\n",
    "* `sum` â†’ identifier\n",
    "* `=` â†’ assignment operator\n",
    "* `a` â†’ identifier\n",
    "* `+` â†’ operator\n",
    "* `b` â†’ identifier\n",
    "* `;` â†’ delimiter\n",
    "\n",
    "### ğŸ“Œ Main Functions of Lexical Analyzer:\n",
    "\n",
    "1. **Token Generation** â€“ Converts characters â†’ tokens.\n",
    "2. **Remove whitespace & comments** â€“ Not needed by parser.\n",
    "3. **Interface with Symbol Table** â€“ For identifiers (variables, functions, etc.).\n",
    "4. **Error Detection** â€“ Reports invalid characters or patterns.\n",
    "5. **Pass tokens to Parser** â€“ Works as input to syntax analysis.\n",
    "\n",
    "âœ… **In simple terms:** It works like a **â€œword scannerâ€** in a compiler.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ **Input Buffering**\n",
    "\n",
    "Problem: Reading **one character at a time** from source is **slow**.\n",
    "\n",
    "### ğŸ“Œ Solution: **Buffering with Sentinels**\n",
    "\n",
    "* Use **two buffers of equal size (say N characters each)**.\n",
    "* While one buffer is being processed, the other is filled from the source program.\n",
    "* Two pointers are used:\n",
    "\n",
    "  * **Lexeme Begin** â†’ Start of token.\n",
    "  * **Forward Pointer** â†’ Moves ahead to find token end.\n",
    "* **Sentinel character (EOF marker)** is placed at end of buffer â†’ avoids repeated end-of-file checks.\n",
    "\n",
    "ğŸ‘‰ **Benefit:** Fast reading & smooth token recognition.\n",
    "\n",
    "âœ… **Analogy:** Like reading a book with **two bookmarks** â€“ one marks the start of a word, and the other scans until the word ends.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ **Specification of Tokens**\n",
    "\n",
    "A **token** is a pair:\n",
    "\n",
    "* **Token Name** â†’ General classification (identifier, keyword, constant, operator).\n",
    "* **Attribute Value** â†’ Points to symbol table entry (extra info like name, type).\n",
    "\n",
    "ğŸ‘‰ Example:\n",
    "Code: `float price = 99;`\n",
    "\n",
    "* `float` â†’ Token: KEYWORD\n",
    "* `price` â†’ Token: IDENTIFIER (attribute: {name: price, type: float})\n",
    "* `=` â†’ ASSIGNMENT OPERATOR\n",
    "* `99` â†’ CONSTANT\n",
    "\n",
    "### ğŸ“Œ Common Token Classes:\n",
    "\n",
    "* **Keywords** â€“ `int, float, while, if`\n",
    "* **Identifiers** â€“ user-defined names (`sum, count`)\n",
    "* **Constants** â€“ numeric (`10, 3.14`) or character (`'a'`)\n",
    "* **Operators** â€“ `+ - * / = < >`\n",
    "* **Delimiters** â€“ `; , ( ) { }`\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ **Recognition of Tokens**\n",
    "\n",
    "Lexical Analyzer must **recognize tokens efficiently**.\n",
    "This is done using **Regular Expressions (RE)** + **Finite Automata (FA)**.\n",
    "\n",
    "### ğŸ“Œ Steps:\n",
    "\n",
    "1. Write **RE** for token patterns.\n",
    "2. Convert RE â†’ **NFA (Non-deterministic Finite Automata)**.\n",
    "3. Convert NFA â†’ **DFA (Deterministic FA)**.\n",
    "4. Minimize DFA â†’ Faster recognition.\n",
    "5. Implement DFA in code â†’ Token recognition.\n",
    "\n",
    "ğŸ‘‰ **Examples of RE for tokens:**\n",
    "\n",
    "* Identifier: `[a-zA-Z_][a-zA-Z0-9_]*`\n",
    "* Number: `[0-9]+`\n",
    "* Relational Operator: `< | <= | > | >= | == | !=`\n",
    "\n",
    "âœ… **In simple terms:** Lexical Analyzer works like a **pattern-matching machine**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ **Lex Tool (Lex/Flex)**\n",
    "\n",
    "The **Lex Tool** (or **Flex**) is a **software tool** to generate lexical analyzers automatically.\n",
    "\n",
    "### ğŸ“Œ How it Works:\n",
    "\n",
    "1. Programmer specifies token patterns (using RE).\n",
    "2. Lex generates a **C program** for the scanner.\n",
    "3. The scanner reads source code and produces tokens.\n",
    "\n",
    "### ğŸ“Œ Structure of a Lex Program:\n",
    "\n",
    "```lex\n",
    "%{\n",
    "/* Declarations (C code, headers) */\n",
    "#include <stdio.h>\n",
    "%}\n",
    "%%\n",
    "[0-9]+      { printf(\"NUMBER \"); }  \n",
    "[a-zA-Z]+   { printf(\"WORD \"); }  \n",
    ".           { printf(\"SYMBOL \"); }  \n",
    "%%\n",
    "int main() {\n",
    "   yylex();   // Start lexical analysis\n",
    "   return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "ğŸ‘‰ **Example Execution:**\n",
    "Input:\n",
    "\n",
    "```\n",
    "sum = 100\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "WORD SYMBOL NUMBER\n",
    "```\n",
    "\n",
    "âœ… **Use of Lex Tool:** Saves time in writing manual token recognition.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ¨ **Quick Recap (Lexical Analysis):**\n",
    "\n",
    "* **Role:** Converts characters â†’ tokens, removes spaces/comments, manages symbol table.\n",
    "* **Input Buffering:** Double buffer + sentinels for fast reading.\n",
    "* **Tokens:** Classified into keywords, identifiers, constants, operators, delimiters.\n",
    "* **Recognition:** Regex â†’ NFA â†’ DFA â†’ Minimized DFA.\n",
    "* **Lex Tool:** Automates lexical analyzer creation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103836f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ **Finite Automata in Compiler Design**\n",
    "\n",
    "Lexical analyzers rely heavily on **finite automata** to recognize tokens.\n",
    "They work as **machines that accept or reject strings** based on defined rules (regular expressions).\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ **Regular Expressions â†’ Automata**\n",
    "\n",
    "### ğŸ“Œ Regular Expressions (RE)\n",
    "\n",
    "A **regular expression** is a formula that describes a pattern of strings.\n",
    "ğŸ‘‰ Used to specify the structure of tokens (identifiers, numbers, operators, etc.).\n",
    "\n",
    "**Examples of RE:**\n",
    "\n",
    "* Identifier â†’ `[a-zA-Z_][a-zA-Z0-9_]*`\n",
    "* Number â†’ `[0-9]+`\n",
    "* Relational operator â†’ `< | <= | > | >= | == | !=`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Finite Automata (FA)\n",
    "\n",
    "A **Finite Automaton** is a mathematical machine that recognizes patterns of input.\n",
    "\n",
    "It consists of:\n",
    "\n",
    "1. **States** (finite number, including start and final states)\n",
    "2. **Alphabet (Î£)** â†’ set of input symbols\n",
    "3. **Transition function** (Î´) â†’ defines movement between states\n",
    "4. **Start state (q0)**\n",
    "5. **Final/accepting states (F)**\n",
    "\n",
    "ğŸ‘‰ If FA ends in a final state after reading the whole input â†’ string is **accepted** (valid token).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Types of Automata\n",
    "\n",
    "1. **NFA (Non-deterministic Finite Automaton)**\n",
    "\n",
    "   * For a given input symbol, machine may move to **multiple states**.\n",
    "   * Can have **Îµ (epsilon) transitions** (move without input).\n",
    "   * Easy to construct from regex.\n",
    "\n",
    "2. **DFA (Deterministic Finite Automaton)**\n",
    "\n",
    "   * For a given input symbol, machine moves to **only one state**.\n",
    "   * No epsilon moves.\n",
    "   * Faster in practice.\n",
    "\n",
    "ğŸ‘‰ **In practice:** Regex â†’ NFA â†’ DFA (via conversion) â†’ DFA Minimization.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Conversion: Regular Expression â†’ NFA â†’ DFA\n",
    "\n",
    "**Step 1: Regex â†’ NFA (Thompsonâ€™s Construction)**\n",
    "\n",
    "* Create NFA fragments for each operator:\n",
    "\n",
    "  * `a` â†’ single transition from start to final on `a`\n",
    "  * `AB` â†’ connect NFA of A to NFA of B (concatenation)\n",
    "  * `A|B` â†’ Îµ-transition to A or B (alternation/OR)\n",
    "  * `A*` â†’ loop with Îµ-transitions (Kleene star)\n",
    "\n",
    "**Step 2: NFA â†’ DFA (Subset Construction)**\n",
    "\n",
    "* Compute **Îµ-closure** (all states reachable from a state with Îµ moves).\n",
    "* For each set of NFA states, create a DFA state.\n",
    "* Define transitions for each input symbol.\n",
    "\n",
    "ğŸ‘‰ **Example:**\n",
    "Regex: `(a|b)*abb`\n",
    "\n",
    "* NFA: multiple paths for a/b choices.\n",
    "* DFA: unique deterministic path.\n",
    "* Recognizes strings like: `abb`, `aabb`, `bababb`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ **Minimization of DFA**\n",
    "\n",
    "Why?\n",
    "\n",
    "* DFA created from NFA may have many redundant states.\n",
    "* Minimization makes it **faster & efficient**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ DFA Minimization Algorithm\n",
    "\n",
    "1. **Partition states** into two groups:\n",
    "\n",
    "   * Final states (accepting)\n",
    "   * Non-final states\n",
    "\n",
    "2. **Refine groups**:\n",
    "\n",
    "   * Split groups further if states behave differently for some input.\n",
    "\n",
    "3. **Repeat refinement** until no more splits possible.\n",
    "\n",
    "4. **Merge equivalent states** â†’ get minimized DFA.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Example of Minimization\n",
    "\n",
    "Suppose a DFA has states: `{A, B, C, D, E}`\n",
    "\n",
    "* Final states: `{D, E}`\n",
    "* Non-final states: `{A, B, C}`\n",
    "\n",
    "Check transitions:\n",
    "\n",
    "* If two states always move to the same group â†’ merge them.\n",
    "* After refinement, suppose `{B, C}` are equivalent.\n",
    "\n",
    "ğŸ‘‰ Minimized DFA may have fewer states: `{A, (B,C), D, E}`.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Final Recap:**\n",
    "\n",
    "* **Regular Expressions** describe token patterns.\n",
    "* **Finite Automata** recognize those patterns.\n",
    "* **NFA** is easy to build from regex, then converted to **DFA** for execution.\n",
    "* **DFA Minimization** removes redundant states â†’ efficient lexical analysis.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
