{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module 4: Statistics with R  \n",
    "- **Random Forest**  \n",
    "- **Decision Tree**  \n",
    "- **Normal and Binomial Distributions**  \n",
    "- **Time Series Analysis**  \n",
    "- **Linear and Multiple Regression**  \n",
    "- **Logistic Regression**  \n",
    "- **Survival Analysis**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module 4: Statistics with R**\n",
    "\n",
    "## **1. Random Forest**\n",
    "\n",
    "### **Introduction**\n",
    "Random Forest is an **ensemble learning method** that builds multiple decision trees and combines their outputs to improve accuracy. It is widely used for both **classification** (categorical output) and **regression** (numerical output).\n",
    "\n",
    "### **How Random Forest Works**\n",
    "1. **Bootstrapping**: Creates multiple subsets of the original dataset by randomly selecting samples with replacement.\n",
    "2. **Feature Selection**: At each node of a decision tree, a random subset of features is chosen.\n",
    "3. **Decision Trees**: Builds multiple decision trees using different subsets and features.\n",
    "4. **Voting/Averaging**:\n",
    "   - **For Classification**: Takes the majority vote from the trees.\n",
    "   - **For Regression**: Takes the average prediction of all trees.\n",
    "\n",
    "### **Advantages**\n",
    "✅ Reduces overfitting compared to a single decision tree\n",
    "✅ Works well with large datasets\n",
    "✅ Handles missing values effectively\n",
    "✅ Can be used for feature selection\n",
    "\n",
    "### **Disadvantages**\n",
    "❌ Computationally expensive\n",
    "❌ Requires careful tuning of hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Decision Tree**\n",
    "\n",
    "### **Introduction**\n",
    "A **Decision Tree** is a tree-like model used for decision-making. It recursively splits data based on conditions to arrive at a decision.\n",
    "\n",
    "### **Types of Decision Trees**\n",
    "1. **Classification Trees**: Used for categorical outcomes.\n",
    "2. **Regression Trees**: Used for continuous numerical outcomes.\n",
    "\n",
    "### **How It Works**\n",
    "1. Starts at a **root node**.\n",
    "2. Splits into **branches** based on the most informative feature.\n",
    "3. Ends at **leaf nodes** that contain final decisions.\n",
    "\n",
    "### **Advantages**\n",
    "✅ Easy to interpret and visualize\n",
    "✅ Handles both numeric and categorical data\n",
    "✅ Requires little data preprocessing\n",
    "\n",
    "### **Disadvantages**\n",
    "❌ Prone to overfitting\n",
    "❌ Unstable with small variations in data\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Normal and Binomial Distributions**\n",
    "\n",
    "### **Normal Distribution (Bell Curve)**\n",
    "\n",
    "- Data is **symmetrically distributed** around the mean.\n",
    "- Many natural phenomena follow this distribution (e.g., human heights, test scores).\n",
    "- Defined by **mean (μ)** and **standard deviation (σ)**.\n",
    "\n",
    "#### **Formula**\n",
    "$$\n",
    "P(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "Where:\n",
    "- $ \\mu $ = Mean\n",
    "- $ \\sigma $ = Standard deviation\n",
    "\n",
    "### **Binomial Distribution**\n",
    "\n",
    "- Used for binary outcomes (Success/Failure, Yes/No, Heads/Tails)\n",
    "- Requires two parameters: **number of trials (n)** and **probability of success (p)**\n",
    "\n",
    "#### **Formula**\n",
    "$$\n",
    "P(X=k) = \\binom{n}{k} p^k (1-p)^{(n-k)}\n",
    "$$\n",
    "Where:\n",
    "- $ n $ = Total trials\n",
    "- $ k $ = Successful outcomes\n",
    "- $ p $ = Probability of success\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Time Series Analysis**\n",
    "\n",
    "### **Introduction**\n",
    "Time Series Analysis examines **patterns in data over time** to make predictions.\n",
    "\n",
    "### **Components of Time Series**\n",
    "- **Trend**: Long-term increase/decrease in data\n",
    "- **Seasonality**: Repeating patterns (e.g., sales increasing in holiday season)\n",
    "- **Cyclic Variations**: Fluctuations without fixed frequency\n",
    "- **Irregular Component**: Random variations\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Linear and Multiple Regression**\n",
    "\n",
    "### **Linear Regression**\n",
    "\n",
    "- Predicts a **continuous** dependent variable based on one independent variable.\n",
    "\n",
    "#### **Formula**\n",
    "$$\n",
    "Y = mX + c\n",
    "$$\n",
    "Where:\n",
    "- $ Y $ = Dependent variable\n",
    "- $ X $ = Independent variable\n",
    "- $ m $ = Slope\n",
    "- $ c $ = Intercept\n",
    "\n",
    "### **Multiple Regression**\n",
    "\n",
    "- Extends Linear Regression by considering **multiple independent variables**.\n",
    "\n",
    "#### **Formula**\n",
    "$$\n",
    "Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Logistic Regression**\n",
    "\n",
    "### **Introduction**\n",
    "- Used for **binary classification** (Yes/No, Spam/Not Spam).\n",
    "\n",
    "#### **Formula (Sigmoid Function)**\n",
    "$$\n",
    "P(Y) = \\frac{1}{1+e^{-(b_0 + b_1X_1 + ... + b_nX_n)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Survival Analysis**\n",
    "\n",
    "### **Introduction**\n",
    "Survival Analysis estimates **the time until an event happens** (e.g., patient survival time, machine failure).\n",
    "\n",
    "### **Kaplan-Meier Estimator**\n",
    "- A non-parametric statistic for estimating survival probability over time.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
