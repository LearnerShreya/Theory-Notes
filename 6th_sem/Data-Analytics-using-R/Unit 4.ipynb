{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module 4: Statistics with R  \n",
    "- **Random Forest**  \n",
    "- **Decision Tree**  \n",
    "- **Normal and Binomial Distributions**  \n",
    "- **Time Series Analysis**  \n",
    "- **Linear and Multiple Regression**  \n",
    "- **Logistic Regression**  \n",
    "- **Survival Analysis**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üëâ 1. Random Forest**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**  \n",
    "Random Forest is an **ensemble learning algorithm** that builds multiple decision trees and combines their outputs for improved accuracy and robustness. It is widely used for **classification** (categorical outputs) and **regression** (continuous outputs).\n",
    "\n",
    "Random Forest:\n",
    "- reduces **overfitting**\n",
    "- improves **accuracy**\n",
    "- works well on large datasets.  \n",
    "\n",
    "---\n",
    "\n",
    "## **How Random Forest Works (Step-by-Step)**  \n",
    "\n",
    "1. **Bootstrapping (Random Sampling with Replacement):**  \n",
    "   - Multiple subsets of data are created by randomly selecting rows from the dataset (some data points may be repeated).  \n",
    "\n",
    "2. **Random Feature Selection at Each Split:**  \n",
    "   - Unlike decision trees, which consider all features, Random Forest selects a **random subset of features** at each split.  \n",
    "\n",
    "3. **Building Multiple Decision Trees:**  \n",
    "   - Each subset is used to train a separate decision tree independently.  \n",
    "\n",
    "4. **Aggregation of Outputs:**  \n",
    "   - **For Classification**: The majority class predicted by the trees is chosen (voting mechanism).  \n",
    "   - **For Regression**: The final output is the **average of all predictions** from the trees.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Explanation**  \n",
    "\n",
    "### **1. Classification in Random Forest**  \n",
    "Each tree gives an output $C_i$, and the final prediction is determined by:  \n",
    "\n",
    "$$\n",
    "\\hat{C} = \\text{Mode} (C_1, C_2, ..., C_n)\n",
    "$$\n",
    "\n",
    "where Mode represents the most frequently occurring class.\n",
    "\n",
    "### **2. Regression in Random Forest**  \n",
    "For regression tasks, the final prediction is:  \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n",
    "$$\n",
    "\n",
    "where $y_i$ is the prediction from the $i^{th}$ tree.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Random Forest**  \n",
    "‚úÖ **Handles Missing Values:** Can handle missing data by maintaining multiple decision paths.  \n",
    "‚úÖ **Reduces Overfitting:** Unlike a single decision tree, it prevents overfitting by averaging multiple trees.  \n",
    "‚úÖ **Works Well with Large Datasets:** Efficient even for large datasets with many features.  \n",
    "‚úÖ **Feature Importance:** Helps in selecting the most important features in a dataset.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of Random Forest**  \n",
    "‚ùå **Computationally Expensive:** Training a large number of trees requires time and memory.  \n",
    "‚ùå **Less Interpretability:** Difficult to interpret compared to a single decision tree.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameters in Random Forest**  \n",
    "\n",
    "| Hyperparameter  | Description |\n",
    "|----------------|------------|\n",
    "| **ntree**      | Number of decision trees (higher = more accuracy but slower computation). |\n",
    "| **mtry**       | Number of features considered at each split (smaller values lead to more diverse trees). |\n",
    "| **nodesize**   | Minimum number of samples required at a leaf node. |\n",
    "| **max_depth**  | Maximum depth of each tree (prevents overfitting). |\n",
    "\n",
    "---\n",
    "\n",
    "## **Real-Life Applications of Random Forest**  \n",
    "\n",
    "üìå **Medical Diagnosis:** Used to predict diseases like cancer based on patient data.  \n",
    "üìå **Financial Fraud Detection:** Helps in detecting fraudulent transactions.  \n",
    "üìå **E-commerce Recommendations:** Suggests products based on user activity.  \n",
    "üìå **Stock Market Prediction:** Analyzes past stock trends to predict future prices.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison: Decision Tree vs. Random Forest**  \n",
    "\n",
    "| Feature           | Decision Tree | Random Forest |\n",
    "|------------------|--------------|--------------|\n",
    "| **Overfitting**  | High         | Low         |\n",
    "| **Accuracy**     | Moderate     | High        |\n",
    "| **Speed**        | Fast         | Slower      |\n",
    "| **Interpretability** | Easy     | Hard       |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**  \n",
    "- **Random Forest** is an improved version of Decision Trees, reducing overfitting by combining multiple trees.  \n",
    "- It is widely used in **classification, regression, feature selection, and anomaly detection**.  \n",
    "- While computationally expensive, it provides **high accuracy and stability**, making it one of the most powerful machine learning algorithms. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üëâ 2. Decision Tree**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**  \n",
    "A **Decision Tree** is a supervised learning algorithm used for **classification** and **regression** tasks. It mimics human decision-making by breaking down a dataset into smaller subsets using a tree-like model of decisions.  \n",
    "\n",
    "It is based on **if-else conditions**, where \n",
    "- **Root Node**: The starting point.\n",
    "- **Internal Nodes**: Decision points.\n",
    "- **Branches**: Outcomes of decisions.\n",
    "- **Leaf Nodes**: Final prediction.  \n",
    "\n",
    "\n",
    "\n",
    "```text\n",
    "                  [Age < 40?]\n",
    "                  /         \\\n",
    "               Yes           No\n",
    "              /               \\\n",
    "     [Income = High?]      Buys = Yes\n",
    "         /     \\\n",
    "      No        Yes\n",
    "     /            \\\n",
    "Buys = No      Buys = Yes\n",
    "```\n",
    "\n",
    "- **If Age < 40**  \n",
    "  - and **Income is High ‚Üí Buys = Yes**  \n",
    "  - else ‚Üí **Buys = No**\n",
    "\n",
    "- **If Age ‚â• 40 ‚Üí Buys = Yes**\n",
    "\n",
    "---\n",
    "\n",
    "## **How Decision Tree Works (Step-by-Step)**  \n",
    "\n",
    "1. **Choose the Best Splitting Feature**  \n",
    "   - The dataset is split based on the most significant feature using measures like **Gini Index** or **Entropy (Information Gain)**.  \n",
    "\n",
    "2. **Create Decision Nodes and Branches**  \n",
    "   - Based on the split, different branches are formed, leading to further decisions.  \n",
    "\n",
    "3. **Continue Splitting Until a Stopping Condition is Met**  \n",
    "   - The splitting process stops when:  \n",
    "     - All data points in a node belong to the same class.  \n",
    "     - A predefined depth is reached.  \n",
    "\n",
    "4. **Make Predictions**  \n",
    "   - For **classification**, the majority class in a leaf node is the output.  \n",
    "   - For **regression**, the average of all values in the leaf node is the output.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Explanation**  \n",
    "\n",
    "### **1. Splitting Criteria in Decision Trees**  \n",
    "The best split is selected using the following metrics:  \n",
    "\n",
    "#### **Entropy & Information Gain (Used in ID3 Algorithm)**  \n",
    "- **Entropy (H)** measures uncertainty in data:  \n",
    "  $$\n",
    "  H(S) = - \\sum p_i \\log_2 p_i\n",
    "  $$  \n",
    "  where $p_i$ is the probability of each class.  \n",
    "\n",
    "- **Information Gain (IG)** is the reduction in entropy after splitting:  \n",
    "  $$\n",
    "  IG = H(Parent) - \\sum \\frac{|Child|}{|Parent|} H(Child)\n",
    "  $$\n",
    "\n",
    "#### **Gini Index (Used in CART Algorithm)**  \n",
    "- Measures impurity in data:  \n",
    "  $$\n",
    "  Gini = 1 - \\sum p_i^2\n",
    "  $$  \n",
    "  where $p_i$ is the proportion of class $i$.  \n",
    "- Lower Gini value means a purer split.  \n",
    "\n",
    "#### **Chi-Square Test (Used in CHAID Algorithm)**  \n",
    "- Measures statistical independence between features.  \n",
    "- The **Chi-Square ($\\chi^2$)** formula is used to test the independence or goodness-of-fit between categorical variables in statistics.\n",
    "\n",
    "üí° **Chi-Square Test Formula:**\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\chi^2$ = Chi-square statistic  \n",
    "- $O$ = Observed frequency  \n",
    "- $E$ = Expected frequency  \n",
    "- $\\sum$ = Summation over all categories\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Use Cases:**\n",
    "- **Goodness-of-Fit Test** ‚Äì To see if a sample matches a population.\n",
    "- **Test of Independence** ‚Äì To check if two categorical variables are related.\n",
    "\n",
    "---\n",
    "\n",
    "## **Types of Decision Trees**  \n",
    "\n",
    "| Type                  | Description                                      |\n",
    "|-----------------------|--------------------------------------------------|\n",
    "| **Classification Tree** | Used when the target variable is categorical.    |\n",
    "| **Regression Tree**     | Used when the target variable is continuous.     |\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Decision Trees**  \n",
    "‚úÖ **Easy to understand and visualize.**  \n",
    "‚úÖ **Handles both numerical and categorical data.**  \n",
    "‚úÖ **No need for feature scaling (e.g., normalization).**  \n",
    "‚úÖ **Requires little data preprocessing.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Disadvantages of Decision Trees**  \n",
    "‚ùå **Prone to overfitting:** Small changes in data can change the tree structure.  \n",
    "‚ùå **Greedy algorithm:** May not find the best split globally.  \n",
    "‚ùå **Biased for dominant classes:** If one class has a higher proportion, it may dominate.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Hyperparameters in Decision Trees**  \n",
    "\n",
    "| Hyperparameter       | Description                                      |\n",
    "|----------------------|--------------------------------------------------|\n",
    "| **max_depth**        | Maximum depth of the tree (controls overfitting).|\n",
    "| **min_samples_split**| Minimum samples required to split a node.        |\n",
    "| **min_samples_leaf** | Minimum samples required in a leaf node.         |\n",
    "| **criterion**        | Metric for choosing the best split (e.g., ‚Äògini‚Äô or ‚Äòentropy‚Äô). |\n",
    "\n",
    "---\n",
    "\n",
    "## **Real-Life Applications of Decision Trees**  \n",
    "\n",
    "üìå **Medical Diagnosis:** Predicting diseases based on symptoms.  \n",
    "üìå **Loan Approval:** Assessing whether a person qualifies for a loan.  \n",
    "üìå **Fraud Detection:** Identifying fraudulent transactions in banking.  \n",
    "üìå **Customer Segmentation:** Grouping customers based on purchase behavior.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison: Decision Tree vs. Random Forest**  \n",
    "\n",
    "| Feature              | Decision Tree | Random Forest |\n",
    "|----------------------|----------------|----------------|\n",
    "| **Overfitting**      | High           | Low            |\n",
    "| **Accuracy**         | Moderate       | High           |\n",
    "| **Speed**            | Fast           | Slower         |\n",
    "| **Interpretability** | Easy           | Hard           |\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**  \n",
    "- **Decision Trees** are intuitive, easy to interpret, and useful for both classification and regression tasks.  \n",
    "- They may overfit data, but techniques like **pruning** or using **Random Forest** help improve performance.  \n",
    "- Commonly used in **finance, healthcare, and customer segmentation**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üëâ 3. Normal and Binomial Distributions**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Normal Distribution**  \n",
    "\n",
    "### **Introduction**  \n",
    "The **Normal Distribution**, also known as the **Gaussian distribution**, is a continuous probability distribution that is symmetric and bell-shaped. It is widely used in statistics, finance, and machine learning to model natural phenomena like height, weight, IQ scores, and stock prices.\n",
    "\n",
    "---\n",
    "\n",
    "### **Characteristics of Normal Distribution**  \n",
    "‚úÖ **Bell-shaped and symmetric** around the mean.  \n",
    "‚úÖ **Mean (Œº), Median, and Mode are equal.**  \n",
    "‚úÖ **Follows the Empirical Rule (68-95-99.7 Rule).**  \n",
    "‚úÖ **Defined by two parameters:**  \n",
    "   - **Mean (Œº)** ‚Äì The central value.  \n",
    "   - **Standard Deviation (œÉ)** ‚Äì The spread of the data.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Probability Density Function (PDF)**  \n",
    "The probability density function (PDF) of a normal distribution is:  \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ \\mu $ = mean  \n",
    "- $ \\sigma $ = standard deviation  \n",
    "- $ e $ = Euler‚Äôs number (~2.718)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Empirical Rule (68-95-99.7 Rule)**  \n",
    "- **68%** of data lies within **1 standard deviation** ($\\mu \\pm 1\\sigma$).  \n",
    "- **95%** of data lies within **2 standard deviations** ($\\mu \\pm 2\\sigma$).  \n",
    "- **99.7%** of data lies within **3 standard deviations** ($\\mu \\pm 3\\sigma$).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Real-Life Applications of Normal Distribution**  \n",
    "üìå **Height and Weight of People:** Most people‚Äôs height follows a normal distribution.  \n",
    "üìå **IQ Scores:** IQ follows a normal distribution with a mean of 100 and $ \\sigma \\approx 15 $.  \n",
    "üìå **Stock Market Returns:** Daily stock returns are approximately normal.  \n",
    "üìå **Measurement Errors:** Errors in experiments tend to be normally distributed.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Binomial Distribution**  \n",
    "\n",
    "### **Introduction**  \n",
    "The **Binomial Distribution** is a discrete probability distribution that represents the number of successes in a fixed number of independent trials, where each trial has two possible outcomes (**success/failure**).  \n",
    "\n",
    "It is commonly used in scenarios like coin flips, pass/fail exams, and customer purchases.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Characteristics of Binomial Distribution**  \n",
    "‚úÖ **Only two outcomes per trial:** Success (1) or Failure (0).  \n",
    "‚úÖ **Fixed number of trials (n).**  \n",
    "‚úÖ **Constant probability of success (p) for each trial.**  \n",
    "‚úÖ **Trials are independent.**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Probability Mass Function (PMF)**  \n",
    "The probability of getting exactly **k successes** in **n trials** is given by:  \n",
    "\n",
    "$$\n",
    "P(X = k) = \\binom{n}{k} p^k (1 - p)^{n-k}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ n $ = number of trials  \n",
    "- $ k $ = number of successes  \n",
    "- $ p $ = probability of success  \n",
    "- $ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} $ (Binomial coefficient)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Real-Life Applications of Binomial Distribution**  \n",
    "üìå **Coin Tossing:** Probability of getting heads 5 times in 10 flips.  \n",
    "üìå **Quality Control:** Probability of 3 defective products in a batch of 10.  \n",
    "üìå **Customer Purchases:** Probability that 4 out of 10 customers make a purchase.  \n",
    "üìå **Medical Trials:** Probability that a drug is effective for 6 out of 10 patients.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison: Normal vs. Binomial Distribution**  \n",
    "\n",
    "| Feature           | Normal Distribution | Binomial Distribution |\n",
    "|------------------|--------------------|----------------------|\n",
    "| **Type**         | Continuous         | Discrete |\n",
    "| **Shape**        | Bell-shaped        | Varies (Skewed for small trials, symmetric for large trials) |\n",
    "| **Parameters**   | Mean (Œº), Standard Deviation (œÉ) | Number of trials (n), Probability of success (p) |\n",
    "| **Examples**     | Heights, IQ scores, Stock prices | Coin tosses, Exam scores, Customer purchases |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| **Aspect**                     | **Normal Distribution**                                                                 | **Binomial Distribution**                                                              |\n",
    "|-------------------------------|-----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
    "| **Type**                      | Continuous distribution                                                                 | Discrete distribution                                                                 |\n",
    "| **Shape**                     | Bell-shaped, symmetric                                                                  | Skewed or symmetric depending on probability                                          |\n",
    "| **Parameters**                | Mean (Œº), Standard Deviation (œÉ)                                                        | Number of trials (n), Probability of success (p)                                      |\n",
    "| **Domain**                    | All real numbers (‚àí‚àû to +‚àû)                                                             | Only integers from 0 to n                                                             |\n",
    "| **Example Use Case**          | Heights of people, marks in exam, IQ scores                                            | Tossing a coin, number of defective products, survey success/failure                  |\n",
    "| **R Function to Generate Data** | `rnorm(n, mean, sd)`                                                                   | `rbinom(n, size, prob)`                                                               |\n",
    "| **R Function for Density**    | `dnorm(x, mean, sd)`                                                                    | `dbinom(x, size, prob)`                                                               |\n",
    "| **R Plot Example**            | `plot(x, dnorm(x))` ‚Äì continuous curve                                                  | `barplot(dbinom(0:n, size, prob))` ‚Äì bars for each value                             |\n",
    "| **Central Limit Theorem**     | Theoretical foundation ‚Äì binomial distribution approaches normal if n is large         | Approximates normal distribution when n is large and p is not near 0 or 1            |\n",
    "| **Probability Between Values**| Can find probability over an interval (e.g., P(5 < X < 10))                             | Probabilities are exact for integers (e.g., P(X = 3))                                |\n",
    "| **Application in R**          | Used with `dnorm()`, `pnorm()` for probability and density curves                      | Used with `dbinom()`, `pbinom()` for probability mass functions                      |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**  \n",
    "- **Normal Distribution** is used for **continuous** data with a bell curve shape, common in real-world applications like height and test scores.  \n",
    "- **Binomial Distribution** is used for **discrete** data with a fixed number of trials, common in probability-based scenarios like coin flips and medical trials.  \n",
    "- As **n increases**, the Binomial Distribution **approaches the Normal Distribution** (Central Limit Theorem).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üëâ 4. Time Series Analysis**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Time Series Analysis**  \n",
    "A **Time Series** is a sequence of data points collected or recorded at specific time intervals. Time series analysis involves analyzing trends, patterns, and seasonal effects in data over time to make forecasts and informed decisions.\n",
    "\n",
    "### **Examples of Time Series Data:**  \n",
    "üìå **Stock Market Prices** ‚Äì Daily fluctuations in stock prices.  \n",
    "üìå **Weather Data** ‚Äì Temperature readings recorded hourly/daily.  \n",
    "üìå **Sales Data** ‚Äì Monthly or yearly revenue of a company.  \n",
    "üìå **Website Traffic** ‚Äì Daily number of visitors to a website.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Components of Time Series Data**  \n",
    "A time series typically consists of the following components:\n",
    "\n",
    "### **1. Trend (Tt)**\n",
    "- The general **direction** in which data is moving over time (increasing, decreasing, or stable).\n",
    "- **Example:** Rising housing prices over the years.\n",
    "\n",
    "### **2. Seasonality (St)**\n",
    "- Patterns that **repeat at regular intervals** (daily, monthly, yearly).\n",
    "- **Example:** Increased ice cream sales in summer.\n",
    "\n",
    "### **3. Cyclic Patterns (Ct)**\n",
    "- **Long-term fluctuations** that repeat but do not have a fixed period.\n",
    "- **Example:** Economic cycles of boom and recession.\n",
    "\n",
    "### **4. Random/Irregular Component (Et)**\n",
    "- **Unpredictable variations** in data due to external factors.\n",
    "- **Example:** Sudden drop in sales due to a pandemic.\n",
    "\n",
    "### **Mathematical Representation of a Time Series:**  \n",
    "A time series is represented as:\n",
    "\n",
    "$$\n",
    "Y_t = T_t + S_t + C_t + E_t\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $Y_t$ = Observed Value  \n",
    "- $T_t$ = Trend  \n",
    "- $S_t$ = Seasonal Component  \n",
    "- $C_t$ = Cyclical Component  \n",
    "- $E_t$ = Random Error  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Types of Time Series Models**  \n",
    "There are **two main types** of time series models:\n",
    "\n",
    "### **1. Additive Model:**  \n",
    "$$\n",
    "Y_t = T_t + S_t + C_t + E_t\n",
    "$$\n",
    "- Used when seasonal variations are **constant** over time.\n",
    "\n",
    "### **2. Multiplicative Model:**  \n",
    "$$\n",
    "Y_t = T_t \\times S_t \\times C_t \\times E_t\n",
    "$$\n",
    "- Used when seasonal variations **increase/decrease proportionally** with time.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Time Series Analysis Techniques**  \n",
    "\n",
    "### **1. Moving Average (MA)**\n",
    "- Used to smooth fluctuations in data and identify trends.\n",
    "- **Simple Moving Average (SMA):**\n",
    "  $$\n",
    "  SMA = \\frac{(X_1 + X_2 + ... + X_n)}{n}\n",
    "  $$\n",
    "- **Example:** Finding a 7-day moving average of stock prices.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Exponential Smoothing**  \n",
    "- Assigns more weight to recent observations.\n",
    "- **Single Exponential Smoothing (SES):**  \n",
    "  $$\n",
    "  S_t = \\alpha Y_t + (1 - \\alpha) S_{t-1}\n",
    "  $$\n",
    "  where $\\alpha$ (smoothing factor) is between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Autoregressive Integrated Moving Average (ARIMA)**\n",
    "- **ARIMA(p, d, q)** is a powerful forecasting model where:  \n",
    "  - **p** = Number of lag observations (AR: Autoregression).  \n",
    "  - **d** = Differencing order (I: Integrated).  \n",
    "  - **q** = Moving average order (MA: Moving Average).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Seasonal Decomposition of Time Series (STL Decomposition)**  \n",
    "- Breaks down a time series into **trend, seasonal, and residual components**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Forecasting in Time Series Analysis**  \n",
    "Forecasting helps predict future values based on historical data.\n",
    "\n",
    "### **Common Forecasting Methods:**\n",
    "1. **Na√Øve Forecasting** ‚Äì Assumes future values will be the same as the last observed value.  \n",
    "2. **Moving Average Forecasting** ‚Äì Uses past averages to predict future trends.  \n",
    "3. **Exponential Smoothing** ‚Äì Assigns different weights to past data.  \n",
    "4. **ARIMA Model** ‚Äì Advanced model for complex time series forecasting.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Performance Evaluation of Time Series Models**  \n",
    "To evaluate the accuracy of a model, we use:\n",
    "\n",
    "üìå **Mean Absolute Error (MAE):**\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum |Y_t - \\hat{Y_t}|\n",
    "$$\n",
    "\n",
    "üìå **Mean Squared Error (MSE):**\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum (Y_t - \\hat{Y_t})^2\n",
    "$$\n",
    "\n",
    "üìå **Root Mean Squared Error (RMSE):**\n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "üìå **Mean Absolute Percentage Error (MAPE):**\n",
    "$$\n",
    "MAPE = \\frac{1}{n} \\sum \\left( \\frac{|Y_t - \\hat{Y_t}|}{Y_t} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Real-Life Applications of Time Series Analysis**  \n",
    "üìå **Stock Market Prediction** ‚Äì ARIMA models forecast stock prices.  \n",
    "üìå **Weather Forecasting** ‚Äì Seasonal models predict temperature and rainfall.  \n",
    "üìå **Sales Forecasting** ‚Äì Helps businesses plan inventory and marketing.  \n",
    "üìå **Energy Consumption Analysis** ‚Äì Power grids use time series to optimize electricity supply.  \n",
    "\n",
    "---\n",
    "\n",
    "## **8. Summary Table**  \n",
    "\n",
    "| **Concept**               | **Description** |\n",
    "|---------------------------|----------------|\n",
    "| **Trend**                 | Long-term movement in data (upward/downward). |\n",
    "| **Seasonality**           | Repeating patterns (daily, monthly, yearly). |\n",
    "| **Cyclic Patterns**       | Irregular but long-term fluctuations. |\n",
    "| **Moving Average (MA)**   | Smoothing method to remove short-term fluctuations. |\n",
    "| **Exponential Smoothing** | More weight to recent values for forecasting. |\n",
    "| **ARIMA Model**           | Advanced model for time series forecasting. |\n",
    "| **Decomposition**         | Splits data into trend, seasonality, and noise. |\n",
    "| **Forecasting**           | Predicting future data points. |\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Conclusion**  \n",
    "- **Time Series Analysis** is crucial for understanding patterns in data over time.  \n",
    "- **Techniques like moving averages, exponential smoothing, and ARIMA** help make accurate predictions.  \n",
    "- **Performance metrics** ensure model accuracy.  \n",
    "- **Real-world applications** include forecasting stock prices, sales, and weather.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üëâ 5. Linear and Multiple Regression**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Regression Analysis**  \n",
    "Regression analysis is a **statistical method** used to understand relationships between variables and make predictions. It helps in determining how a dependent variable (outcome) changes when one or more independent variables (predictors) are modified.\n",
    "\n",
    "### **Types of Regression:**  \n",
    "üìå **Linear Regression** ‚Äì Relationship between one independent variable and one dependent variable.  \n",
    "üìå **Multiple Regression** ‚Äì Relationship between multiple independent variables and one dependent variable.  \n",
    "üìå **Logistic Regression** ‚Äì Used for categorical outcomes (e.g., Yes/No).  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Linear Regression**  \n",
    "### **Definition:**  \n",
    "Linear regression is used to model the relationship between a **dependent variable (Y)** and **one independent variable (X)** using a straight line.\n",
    "\n",
    "### **Equation of a Simple Linear Regression Model:**  \n",
    "$$\n",
    "Y = mX + c + e\n",
    "$$  \n",
    "where:  \n",
    "- $Y$ = Dependent variable (output/predicted value).  \n",
    "- $X$ = Independent variable (input).  \n",
    "- $m$ = Slope of the regression line (rate of change).  \n",
    "- $c$ = Intercept (value of Y when X = 0).  \n",
    "- $e$ = Error term (unexplained variations in Y).  \n",
    "\n",
    "### **Example:**  \n",
    "- Predicting house prices based on square footage.  \n",
    "- Predicting student scores based on study hours.  \n",
    "\n",
    "### **Assumptions of Linear Regression:**  \n",
    "‚úÖ **Linearity** ‚Äì Relationship between X and Y is linear.  \n",
    "‚úÖ **Independence** ‚Äì Observations are independent.  \n",
    "‚úÖ **Homoscedasticity** ‚Äì Constant variance of errors.  \n",
    "‚úÖ **Normality** ‚Äì Errors follow a normal distribution.  \n",
    "\n",
    "### **Visual Representation of Linear Regression:**  \n",
    "A scatter plot with a best-fit line showing the relationship between **X** (independent variable) and **Y** (dependent variable).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Multiple Regression**  \n",
    "### **Definition:**  \n",
    "Multiple regression is an extension of linear regression that includes **multiple independent variables** to predict a single dependent variable.\n",
    "\n",
    "### **Equation of Multiple Regression:**  \n",
    "$$\n",
    "Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n + e\n",
    "$$  \n",
    "where:  \n",
    "- $Y$ = Dependent variable.  \n",
    "- $X_1, X_2, ..., X_n$ = Multiple independent variables.  \n",
    "- $b_0$ = Intercept.  \n",
    "- $b_1, b_2, ..., b_n$ = Coefficients for each independent variable.  \n",
    "- $e$ = Error term.  \n",
    "\n",
    "### **Example:**  \n",
    "- Predicting house prices based on **square footage, number of bedrooms, and location**.  \n",
    "- Estimating employee salaries based on **experience, education, and skill level**.  \n",
    "\n",
    "### **When to Use Multiple Regression?**  \n",
    "üìå When **one factor is not enough** to predict the outcome.  \n",
    "üìå When **multiple factors** contribute to the dependent variable.  \n",
    "\n",
    "### **Assumptions of Multiple Regression:**  \n",
    "‚úÖ **No Multicollinearity** ‚Äì Independent variables should not be highly correlated.  \n",
    "‚úÖ **Linearity** ‚Äì Relationship between independent and dependent variables is linear.  \n",
    "‚úÖ **Homoscedasticity** ‚Äì Variance of errors is constant.  \n",
    "‚úÖ **Normal Distribution of Errors** ‚Äì Errors should be normally distributed.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Model Evaluation Metrics**  \n",
    "To measure how well a regression model fits the data, we use:\n",
    "\n",
    "### **1. R-Squared ($R^2$)**  \n",
    "- Measures the proportion of variance in Y explained by X.  \n",
    "- **Ranges from 0 to 1** (closer to 1 = better fit).  \n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (Y - \\hat{Y})^2}{\\sum (Y - \\bar{Y})^2}\n",
    "$$  \n",
    "\n",
    "üìå **Example:** If $R^2 = 0.85$, it means **85% of the variation in Y** is explained by X.  \n",
    "\n",
    "### **2. Mean Absolute Error (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum |Y - \\hat{Y}|\n",
    "$$  \n",
    "Measures the average absolute difference between actual and predicted values. Lower is better.  \n",
    "\n",
    "### **3. Mean Squared Error (MSE)**  \n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum (Y - \\hat{Y})^2\n",
    "$$  \n",
    "Punishes larger errors more than MAE.  \n",
    "\n",
    "### **4. Root Mean Squared Error (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$  \n",
    "Gives an idea of the error magnitude in original units.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Real-World Applications of Regression Analysis**  \n",
    "üìå **Finance** ‚Äì Predicting stock prices based on economic indicators.  \n",
    "üìå **Healthcare** ‚Äì Estimating disease risk based on patient history.  \n",
    "üìå **Marketing** ‚Äì Analyzing the effect of advertising on sales.  \n",
    "üìå **Education** ‚Äì Predicting student performance based on study time and attendance.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary Table**  \n",
    "\n",
    "| **Concept**                | **Linear Regression** | **Multiple Regression** |\n",
    "|----------------------------|----------------------|--------------------------|\n",
    "| **Number of Independent Variables** | 1 | 2 or more |\n",
    "| **Equation** | $Y = mX + c$ | $Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n$ |\n",
    "| **Example** | Study time ‚Üí Exam Score | Study time + Attendance ‚Üí Exam Score |\n",
    "| **Usage** | Simple relationships | Complex relationships |\n",
    "| **Interpretation** | Change in Y per unit change in X | Change in Y based on multiple factors |\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Conclusion**  \n",
    "- **Linear regression** is useful for understanding the relationship between one independent and one dependent variable.  \n",
    "- **Multiple regression** allows analyzing the effect of multiple factors on an outcome.  \n",
    "- **R-squared, MAE, MSE, and RMSE** help evaluate model performance.  \n",
    "- **Used in various fields** like finance, healthcare, and education.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **üëâ 6. Logistic Regression**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Logistic Regression**  \n",
    "Logistic regression is a **statistical method** used for **classification problems** where the output is **binary or categorical** (e.g., Yes/No, 0/1, Pass/Fail).  \n",
    "\n",
    "üìå **Why Not Use Linear Regression?**  \n",
    "- Linear regression gives continuous values, which are not ideal for classification.  \n",
    "- Logistic regression transforms the output into probabilities between **0 and 1** using a **sigmoid function**.  \n",
    "\n",
    "### **Types of Logistic Regression:**  \n",
    "‚úÖ **Binary Logistic Regression** ‚Äì Two possible outcomes (e.g., Spam/Not Spam).  \n",
    "‚úÖ **Multinomial Logistic Regression** ‚Äì More than two **unordered** categories (e.g., Red, Blue, Green).  \n",
    "‚úÖ **Ordinal Logistic Regression** ‚Äì More than two **ordered** categories (e.g., Low, Medium, High).  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Sigmoid Function (Logistic Function)**  \n",
    "Since logistic regression deals with probabilities, it uses the **sigmoid function** to squash values between **0 and 1**.  \n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ \\sigma(z) $ = Probability value (between 0 and 1).  \n",
    "- $ e $ = Euler‚Äôs number (**2.718**).  \n",
    "- $ z $ = Linear combination of input features.  \n",
    "\n",
    "### **Graph of Sigmoid Function:**  \n",
    "- If $ z $ is **very large**, $ \\sigma(z) $ approaches **1**.  \n",
    "- If $ z $ is **very small**, $ \\sigma(z) $ approaches **0**.  \n",
    "- If $ z = 0 $, $ \\sigma(0) = 0.5 $.  \n",
    "\n",
    "üìå **Interpretation:** The closer $ \\sigma(z) $ is to **1**, the more likely the event is to happen. If it is closer to **0**, the event is unlikely.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Logistic Regression Equation**  \n",
    "Instead of a straight line like in linear regression, logistic regression models a probability using:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-(b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n)}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ p $ = Probability of success.  \n",
    "- $ b_0 $ = Intercept.  \n",
    "- $ b_1, b_2, ..., b_n $ = Coefficients of independent variables.  \n",
    "- $ X_1, X_2, ..., X_n $ = Independent variables (features).  \n",
    "\n",
    "Taking the **logit transformation**:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p}{1-p}\\right) = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n\n",
    "$$\n",
    "\n",
    "üìå **This converts the probability into a linear equation.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Decision Boundary**  \n",
    "Logistic regression classifies data based on a **threshold** (usually **0.5**):  \n",
    "- **If $ p > 0.5 $**, classify as **1 (Yes, Positive, Success, True)**.  \n",
    "- **If $ p < 0.5 $**, classify as **0 (No, Negative, Failure, False)**.  \n",
    "\n",
    "üìå **Example:**  \n",
    "- If $ p = 0.8 $ ‚Üí Predict **Yes** (1).  \n",
    "- If $ p = 0.3 $ ‚Üí Predict **No** (0).  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Loss Function ‚Äì Log Loss (Cross-Entropy Loss)**  \n",
    "Since logistic regression is based on probabilities, it uses **log loss** instead of mean squared error (MSE).  \n",
    "\n",
    "$$\n",
    "Loss = -\\frac{1}{n} \\sum \\left[ y \\log(p) + (1 - y) \\log(1 - p) \\right]\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ y $ = Actual class label (0 or 1).  \n",
    "- $ p $ = Predicted probability.  \n",
    "\n",
    "üìå **Goal:** Minimize log loss to make better predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Model Evaluation Metrics**  \n",
    "After training a logistic regression model, we evaluate its performance using:  \n",
    "\n",
    "### **1. Accuracy**  \n",
    "$$\n",
    "Accuracy = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "### **2. Confusion Matrix**  \n",
    "A table that shows the actual vs. predicted values:  \n",
    "\n",
    "| **Actual/Predicted** | **Predicted: 0** | **Predicted: 1** |\n",
    "|----------------------|------------------|------------------|\n",
    "| **Actual: 0**        | True Negative (TN) | False Positive (FP) |\n",
    "| **Actual: 1**        | False Negative (FN) | True Positive (TP)  |\n",
    "\n",
    "üìå **Key Metrics Derived from the Confusion Matrix:**  \n",
    "\n",
    "- **Precision**: $ \\frac{TP}{TP + FP} $ (How many predicted positives are actually positive?)  \n",
    "- **Recall (Sensitivity)**: $ \\frac{TP}{TP + FN} $ (How many actual positives were correctly predicted?)  \n",
    "- **F1-Score**: Harmonic mean of precision and recall.  \n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "- **ROC Curve & AUC Score**: Measures how well the model distinguishes between classes.  \n",
    "\n",
    "---\n",
    "\n",
    "## **7. Real-World Applications of Logistic Regression**  \n",
    "üìå **Healthcare** ‚Äì Predicting disease risk (Diabetes: Yes/No).  \n",
    "üìå **Finance** ‚Äì Credit card fraud detection (Fraud/Not Fraud).  \n",
    "üìå **Marketing** ‚Äì Customer conversion prediction (Will buy/Will not buy).  \n",
    "üìå **E-commerce** ‚Äì Product recommendation (Interested/Not Interested).  \n",
    "\n",
    "---\n",
    "\n",
    "## **8. Summary Table**  \n",
    "\n",
    "| **Concept**                | **Details** |\n",
    "|----------------------------|-------------|\n",
    "| **Used For**               | Classification Problems |\n",
    "| **Output**                 | Probability (0 to 1) |\n",
    "| **Function Used**          | Sigmoid Function |\n",
    "| **Equation**               | $ \\log(\\frac{p}{1-p}) = b_0 + b_1X_1 + ... + b_nX_n $ |\n",
    "| **Threshold**              | 0.5 (Default) |\n",
    "| **Evaluation Metrics**     | Accuracy, Precision, Recall, F1-Score |\n",
    "| **Loss Function**          | Log Loss (Cross-Entropy) |\n",
    "| **Real-World Use Cases**   | Medical Diagnosis, Fraud Detection, Marketing Analytics |\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Conclusion**  \n",
    "- Logistic Regression is a **classification algorithm** that predicts binary or categorical outcomes.  \n",
    "- It uses the **sigmoid function** to output probabilities between 0 and 1.  \n",
    "- **Thresholding** helps decide the class (0 or 1).  \n",
    "- **Log Loss** is minimized to improve accuracy.  \n",
    "- It is widely used in **healthcare, finance, marketing, and e-commerce**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **üëâ 7. Survival Analysis**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Survival Analysis**  \n",
    "Survival analysis is a **statistical method** used to analyze the **time until an event occurs**. This event could be **death, failure of a system, recovery from an illness, or churn of a customer in business**.  \n",
    "\n",
    "### **Key Features of Survival Analysis:**  \n",
    "‚úÖ Analyzes **time-to-event data**.  \n",
    "‚úÖ Handles **censored data** (when an event hasn‚Äôt happened yet for some subjects).  \n",
    "‚úÖ Estimates **survival probabilities** over time.  \n",
    "\n",
    "üìå **Examples:**  \n",
    "- Medical studies: Time until a patient dies after treatment.  \n",
    "- Business: Time until a customer cancels a subscription.  \n",
    "- Engineering: Time until a machine breaks down.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Important Terminologies in Survival Analysis**  \n",
    "\n",
    "### **1. Event (Failure or Success)**\n",
    "The occurrence of a **specific event** (e.g., death, machine failure, recovery).  \n",
    "\n",
    "### **2. Survival Time (T)**\n",
    "The time from the **start of observation** until the event occurs.  \n",
    "\n",
    "### **3. Censoring**\n",
    "When the exact event time is **unknown**, we call it **censored data**. Types of censoring:  \n",
    "- **Right-censoring**: The event hasn‚Äôt occurred by the end of the study (most common).  \n",
    "- **Left-censoring**: The event happened before observation started.  \n",
    "- **Interval-censoring**: The event happened between two time points but the exact time is unknown.  \n",
    "\n",
    "üìå **Example of Right Censoring:**  \n",
    "A patient is part of a cancer drug trial for **5 years**, but they are still alive at the end. We don‚Äôt know when they might die, so their data is **right-censored**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Survival Function and Hazard Function**  \n",
    "\n",
    "### **1. Survival Function $S(t)$**\n",
    "The probability that the event **has not occurred** by time $t$.  \n",
    "\n",
    "$$\n",
    "S(t) = P(T > t)\n",
    "$$\n",
    "\n",
    "- $S(t)$ is always **between 0 and 1**.  \n",
    "- $S(0) = 1$ (At the start, no one has experienced the event).  \n",
    "- $S(\\infty) = 0$ (Eventually, all subjects will experience the event).  \n",
    "\n",
    "üìå **Interpretation:**  \n",
    "- $S(5) = 0.8$ ‚Üí 80% of individuals **survive beyond 5 years**.  \n",
    "\n",
    "### **2. Hazard Function $h(t)$**\n",
    "The **instantaneous rate** at which events occur, given that the subject **has survived up to time t**.  \n",
    "\n",
    "$$\n",
    "h(t) = \\frac{f(t)}{S(t)}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $f(t)$ = Probability density function of survival times.  \n",
    "- $S(t)$ = Survival function.  \n",
    "\n",
    "üìå **Interpretation:**  \n",
    "- A **higher hazard** means a higher risk of the event occurring at time $t$.  \n",
    "- Example: If $h(5) = 0.2$, the risk of dying at year 5 is **20%**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Methods of Survival Analysis**  \n",
    "\n",
    "### **1. Kaplan-Meier Estimator (KM Curve)**\n",
    "A **non-parametric** method to estimate the survival function.  \n",
    "\n",
    "$$\n",
    "S(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $d_i$ = Number of events at time $t_i$.  \n",
    "- $n_i$ = Number of subjects at risk just before $t_i$.  \n",
    "\n",
    "üìå **How it Works:**  \n",
    "- At each event time, the survival probability is **multiplied** by the probability of surviving past that time.  \n",
    "- The result is a **stepwise decreasing survival curve**.  \n",
    "\n",
    "‚úÖ **Used for:** Comparing two groups (e.g., treatment vs. control).  \n",
    "‚úÖ **Plot:** The **Kaplan-Meier curve** shows survival probabilities over time.  \n",
    "\n",
    "### **2. Log-Rank Test**\n",
    "A **hypothesis test** to compare **two or more survival curves** (e.g., comparing drug A vs. drug B).  \n",
    "\n",
    "- Null Hypothesis ($H_0$): No difference in survival between groups.  \n",
    "- Alternative Hypothesis ($H_1$): A significant difference exists.  \n",
    "\n",
    "If the **p-value < 0.05**, we reject $H_0$ and conclude that there is a **difference in survival** between groups.  \n",
    "\n",
    "### **3. Cox Proportional Hazards Model (Cox Regression)**\n",
    "A **semi-parametric** model that estimates the effect of variables on survival time.  \n",
    "\n",
    "$$\n",
    "h(t) = h_0(t) e^{(b_1X_1 + b_2X_2 + ... + b_nX_n)}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $h_0(t)$ = Baseline hazard.  \n",
    "- $X_1, X_2, ..., X_n$ = Independent variables.  \n",
    "- $b_1, b_2, ..., b_n$ = Regression coefficients.  \n",
    "\n",
    "üìå **Interpretation:**  \n",
    "- If $b_1 > 0$, the variable **increases the hazard (risk of event)**.  \n",
    "- If $b_1 < 0$, the variable **decreases the hazard** (protective factor).  \n",
    "- If $HR = e^{b_1} = 2$, the event is **twice as likely** for that variable.  \n",
    "\n",
    "‚úÖ **Used for:** Finding risk factors in medical research. \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Real-World Applications of Survival Analysis**  \n",
    "üìå **Healthcare** ‚Äì Studying the survival time of cancer patients.  \n",
    "üìå **Business** ‚Äì Predicting customer churn.  \n",
    "üìå **Engineering** ‚Äì Reliability of machine components.  \n",
    "üìå **Finance** ‚Äì Risk assessment for loans.  \n",
    "\n",
    "---\n",
    "\n",
    "## **7. Summary Table**  \n",
    "\n",
    "| **Concept**                 | **Description** |\n",
    "|-----------------------------|----------------|\n",
    "| **Event** | The occurrence of interest (e.g., death, failure, churn). |\n",
    "| **Censoring** | When the event has not occurred yet for some subjects. |\n",
    "| **Survival Function $S(t)$** | Probability of survival beyond time $t$. |\n",
    "| **Hazard Function $h(t)$** | Instantaneous risk of event occurring at $t$. |\n",
    "| **Kaplan-Meier Estimator** | Non-parametric method to estimate survival probability. |\n",
    "| **Log-Rank Test** | Compares survival curves between groups. |\n",
    "| **Cox Regression** | Estimates impact of variables on survival time. |\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Conclusion**  \n",
    "- Survival analysis helps analyze **time-to-event data** and handles **censored cases**.  \n",
    "- **Kaplan-Meier estimator** is used for survival probability estimation.  \n",
    "- **Cox regression** identifies risk factors affecting survival.  \n",
    "- It is widely used in **medicine, business, engineering, and finance**.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
